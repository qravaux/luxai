{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'player_0': array(0, dtype=int32), 'player_1': array(0, dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "env = LuxAIS3GymEnv(numpy_output=True)\n",
    "env.reset()\n",
    "\n",
    "action = dict(\n",
    "                player_0=np.random.randint(0,5,size=(env.env_params.max_units, 3)),\n",
    "                player_1=np.random.randint(0,5,size=(env.env_params.max_units, 3))\n",
    "            )\n",
    "for i in range(10) :\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1880])\n"
     ]
    }
   ],
   "source": [
    "def obs_to_state(obs) :\n",
    "\n",
    "    n_units = len(obs['units']['position'][0])\n",
    "    list_state = []\n",
    "\n",
    "    #Units\n",
    "    list_state.append(torch.from_numpy(obs['units']['position'].astype(float)).flatten()) #position\n",
    "    list_state.append(torch.from_numpy(obs['units']['energy'].astype(float)).flatten()) #energy\n",
    "    list_state.append(torch.from_numpy(obs['units_mask'].astype(float)).flatten()) #unit_mask\n",
    "    \n",
    "    #Map\n",
    "    list_state.append(torch.from_numpy(obs['sensor_mask'].astype(float)).flatten()) #sensor_mask\n",
    "    list_state.append(torch.from_numpy(obs['map_features']['energy'].astype(float)).flatten()) #map_energy\n",
    "    list_state.append(torch.from_numpy(obs['map_features']['tile_type'].astype(float)).flatten()) #map_tile_type\n",
    "\n",
    "    list_state.append(torch.from_numpy(obs['relic_nodes'].astype(float)).flatten()) #relic_nodes\n",
    "    list_state.append(torch.from_numpy(obs['relic_nodes_mask'].astype(float)).flatten()) #relic_nodes_mask\n",
    "\n",
    "    #Game\n",
    "    list_state.append(torch.from_numpy(obs['team_points'].astype(float)).flatten()) #team_points\n",
    "    list_state.append(torch.from_numpy(obs['team_wins'].astype(float)).flatten()) #team_wins\n",
    "\n",
    "    list_state.append(torch.from_numpy(obs['steps'].astype(float)).flatten()) #steps\n",
    "    list_state.append(torch.from_numpy(obs['match_steps'].astype(float)).flatten()) #match_steps\n",
    "    \n",
    "    return torch.cat(list_state)\n",
    "\n",
    "\n",
    "env = LuxAIS3GymEnv(numpy_output=True)\n",
    "obs , _ = env.reset()\n",
    "state = obs_to_state(obs['player_0'])\n",
    "print(state.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module) :\n",
    "    def __init__(self,n_input,n_action,n_units,sap_range) :\n",
    "\n",
    "        super(Policy,self).__init__()\n",
    "\n",
    "        self.n_units = n_units\n",
    "        self.n_action = n_action\n",
    "        self.sap_range = sap_range\n",
    "\n",
    "        self.inputs = nn.Linear(n_input,512,dtype=torch.double)\n",
    "\n",
    "        self.hidden1 = nn.Linear(512,128,dtype=torch.double)\n",
    "        self.hidden2 = nn.Linear(128,32,dtype=torch.double)\n",
    "\n",
    "        self.actor_action = []\n",
    "        self.actor_dx = []\n",
    "        self.actor_dy = []\n",
    "\n",
    "        self.critic_action = []\n",
    "        self.critic_dx = []\n",
    "        self.critic_dy = []\n",
    "\n",
    "        for unit in range(self.n_units) :\n",
    "            self.actor_action.append(nn.Linear(32,self.n_action,dtype=torch.double))\n",
    "            self.actor_dx.append(nn.Linear(32,self.sap_range*2 + 1,dtype=torch.double))\n",
    "            self.actor_dy.append(nn.Linear(32,self.sap_range*2 + 1,dtype=torch.double))\n",
    "\n",
    "        self.critic = nn.Linear(32,1,dtype=torch.double)\n",
    "\n",
    "    def forward(self,x) :\n",
    "\n",
    "        x = F.relu(self.inputs(x))\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "\n",
    "        actor_action = torch.zeros(self.n_units,self.n_action)\n",
    "        actor_dx = torch.zeros(self.n_units,self.sap_range*2 + 1)\n",
    "        actor_dy = torch.zeros(self.n_units,self.sap_range*2 + 1)\n",
    "\n",
    "        for unit in range(self.n_units) :\n",
    "            actor_action[unit] = self.actor_action[unit](F.log_softmax(x,dim=-1))\n",
    "            actor_dx[unit] = self.actor_dx[unit](F.log_softmax(x,dim=-1))\n",
    "            actor_dy[unit] = self.actor_dy[unit](F.log_softmax(x,dim=-1))\n",
    "\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return actor_action,actor_dx,actor_dy,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "n_input = 1880\n",
    "n_units = env.env_params.max_units\n",
    "sap_range = env.env_params.unit_sap_range\n",
    "n_action = 6\n",
    "\n",
    "model = Policy(n_input,n_action,n_units,sap_range=sap_range)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "num_episodes = 2\n",
    "gamma = 0.9\n",
    "count = 0\n",
    "n = 1000\n",
    "victory_bonus = 0\n",
    "\n",
    "match_step = env.env_params.max_steps_in_match + 1\n",
    "len_episode = match_step * env.env_params.match_count_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 7 tensor(4120.6821, grad_fn=<AddBackward0>)\n",
      "9 66 tensor(2683.5974, grad_fn=<AddBackward0>)\n",
      "4 11 tensor(1252.1289, grad_fn=<AddBackward0>)\n",
      "12 1 tensor(242.1427, grad_fn=<AddBackward0>)\n",
      "354 450 tensor(27057.8867, grad_fn=<AddBackward0>)\n",
      "1488 1375 tensor(350554.3125, grad_fn=<AddBackward0>)\n",
      "1529 1259 tensor(327720.6562, grad_fn=<AddBackward0>)\n",
      "282 381 tensor(15270.5625, grad_fn=<AddBackward0>)\n",
      "12 1 tensor(131.1758, grad_fn=<AddBackward0>)\n",
      "2 43 tensor(671.0272, grad_fn=<AddBackward0>)\n",
      "2 1 tensor(202.3920, grad_fn=<AddBackward0>)\n",
      "69 58 tensor(7785.3657, grad_fn=<AddBackward0>)\n",
      "33 100 tensor(3918.5835, grad_fn=<AddBackward0>)\n",
      "39 40 tensor(733.6310, grad_fn=<AddBackward0>)\n",
      "14 35 tensor(3219.2019, grad_fn=<AddBackward0>)\n",
      "4 71 tensor(924.8276, grad_fn=<AddBackward0>)\n",
      "175 148 tensor(5098.7100, grad_fn=<AddBackward0>)\n",
      "177 140 tensor(4621.6445, grad_fn=<AddBackward0>)\n",
      "72 50 tensor(1177.6046, grad_fn=<AddBackward0>)\n",
      "0 12 tensor(57.9554, grad_fn=<AddBackward0>)\n",
      "99 45 tensor(2675.8347, grad_fn=<AddBackward0>)\n",
      "0 52 tensor(873.6766, grad_fn=<AddBackward0>)\n",
      "2 16 tensor(80.1037, grad_fn=<AddBackward0>)\n",
      "4 30 tensor(1387.9763, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(52.2409, grad_fn=<AddBackward0>)\n",
      "33 92 tensor(2228.5837, grad_fn=<AddBackward0>)\n",
      "127 212 tensor(8396.8105, grad_fn=<AddBackward0>)\n",
      "174 193 tensor(8469.0879, grad_fn=<AddBackward0>)\n",
      "33 114 tensor(8051.9771, grad_fn=<AddBackward0>)\n",
      "2 5 tensor(1834.4646, grad_fn=<AddBackward0>)\n",
      "22 100 tensor(2428.8955, grad_fn=<AddBackward0>)\n",
      "1 1 tensor(744.0205, grad_fn=<AddBackward0>)\n",
      "11 22 tensor(197.2657, grad_fn=<AddBackward0>)\n",
      "93 180 tensor(3284.5620, grad_fn=<AddBackward0>)\n",
      "80 18 tensor(680.6876, grad_fn=<AddBackward0>)\n",
      "0 10 tensor(17.3657, grad_fn=<AddBackward0>)\n",
      "112 99 tensor(5319.5532, grad_fn=<AddBackward0>)\n",
      "0 43 tensor(651.3088, grad_fn=<AddBackward0>)\n",
      "30 7 tensor(451.2514, grad_fn=<AddBackward0>)\n",
      "0 36 tensor(600.8911, grad_fn=<AddBackward0>)\n",
      "14 76 tensor(1616.8201, grad_fn=<AddBackward0>)\n",
      "1726 1549 tensor(456135.5312, grad_fn=<AddBackward0>)\n",
      "76 64 tensor(79767.3359, grad_fn=<AddBackward0>)\n",
      "53 30 tensor(1113.1438, grad_fn=<AddBackward0>)\n",
      "27 61 tensor(617.1723, grad_fn=<AddBackward0>)\n",
      "95 96 tensor(56400.7109, grad_fn=<AddBackward0>)\n",
      "47 67 tensor(1026.3921, grad_fn=<AddBackward0>)\n",
      "55 17 tensor(27883.6270, grad_fn=<AddBackward0>)\n",
      "7 19 tensor(3220.7781, grad_fn=<AddBackward0>)\n",
      "45 93 tensor(2065.8376, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(1472.2397, grad_fn=<AddBackward0>)\n",
      "6 18 tensor(67.5717, grad_fn=<AddBackward0>)\n",
      "22 59 tensor(1027.2264, grad_fn=<AddBackward0>)\n",
      "7 8 tensor(58.8430, grad_fn=<AddBackward0>)\n",
      "38 105 tensor(12103.7139, grad_fn=<AddBackward0>)\n",
      "5 6 tensor(92.7511, grad_fn=<AddBackward0>)\n",
      "53 2 tensor(1461.3784, grad_fn=<AddBackward0>)\n",
      "21 28 tensor(86349.4297, grad_fn=<AddBackward0>)\n",
      "58 94 tensor(1884.6763, grad_fn=<AddBackward0>)\n",
      "48 19 tensor(1087.5994, grad_fn=<AddBackward0>)\n",
      "0 12 tensor(2306.8064, grad_fn=<AddBackward0>)\n",
      "2 39 tensor(2396.6091, grad_fn=<AddBackward0>)\n",
      "1 6 tensor(18.2932, grad_fn=<AddBackward0>)\n",
      "62 12 tensor(11875.0078, grad_fn=<AddBackward0>)\n",
      "2 15 tensor(83.5332, grad_fn=<AddBackward0>)\n",
      "5 4 tensor(816.5110, grad_fn=<AddBackward0>)\n",
      "2 22 tensor(200.2091, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(467.8998, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(578.6881, grad_fn=<AddBackward0>)\n",
      "10 4 tensor(555.4506, grad_fn=<AddBackward0>)\n",
      "1 44 tensor(403.1190, grad_fn=<AddBackward0>)\n",
      "1 38 tensor(142.6442, grad_fn=<AddBackward0>)\n",
      "30 55 tensor(38767.8125, grad_fn=<AddBackward0>)\n",
      "351 295 tensor(18573.4453, grad_fn=<AddBackward0>)\n",
      "145 114 tensor(5871.0767, grad_fn=<AddBackward0>)\n",
      "0 3 tensor(1052.6743, grad_fn=<AddBackward0>)\n",
      "40 134 tensor(3184.5940, grad_fn=<AddBackward0>)\n",
      "35 12 tensor(222.7934, grad_fn=<AddBackward0>)\n",
      "21 84 tensor(5566.7275, grad_fn=<AddBackward0>)\n",
      "0 13 tensor(32.2887, grad_fn=<AddBackward0>)\n",
      "1192 1116 tensor(220278.1250, grad_fn=<AddBackward0>)\n",
      "286 346 tensor(16620.9590, grad_fn=<AddBackward0>)\n",
      "134 123 tensor(5650.5312, grad_fn=<AddBackward0>)\n",
      "2 78 tensor(1932.7040, grad_fn=<AddBackward0>)\n",
      "4 51 tensor(860.6960, grad_fn=<AddBackward0>)\n",
      "5 129 tensor(2768.9802, grad_fn=<AddBackward0>)\n",
      "37 68 tensor(4334.4023, grad_fn=<AddBackward0>)\n",
      "85 3 tensor(287.1652, grad_fn=<AddBackward0>)\n",
      "0 20 tensor(17721.5488, grad_fn=<AddBackward0>)\n",
      "67 7 tensor(8811.6406, grad_fn=<AddBackward0>)\n",
      "9 56 tensor(634.3574, grad_fn=<AddBackward0>)\n",
      "95 31 tensor(2558.0027, grad_fn=<AddBackward0>)\n",
      "33 82 tensor(2193.9236, grad_fn=<AddBackward0>)\n",
      "5 1 tensor(6.7001, grad_fn=<AddBackward0>)\n",
      "5 41 tensor(143.7386, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(21.9150, grad_fn=<AddBackward0>)\n",
      "82 27 tensor(2255.1199, grad_fn=<AddBackward0>)\n",
      "0 2 tensor(1402.0323, grad_fn=<AddBackward0>)\n",
      "591 424 tensor(42953.6055, grad_fn=<AddBackward0>)\n",
      "89 76 tensor(1592.6021, grad_fn=<AddBackward0>)\n",
      "59 174 tensor(3199.6111, grad_fn=<AddBackward0>)\n",
      "4 30 tensor(1427.7325, grad_fn=<AddBackward0>)\n",
      "0 3 tensor(0.7084, grad_fn=<AddBackward0>)\n",
      "2 2 tensor(964.6497, grad_fn=<AddBackward0>)\n",
      "533 280 tensor(27903.3594, grad_fn=<AddBackward0>)\n",
      "5 26 tensor(115978.5312, grad_fn=<AddBackward0>)\n",
      "2 1 tensor(2668.2678, grad_fn=<AddBackward0>)\n",
      "220 211 tensor(14563.1143, grad_fn=<AddBackward0>)\n",
      "73 48 tensor(1129.2709, grad_fn=<AddBackward0>)\n",
      "19 49 tensor(5622.2158, grad_fn=<AddBackward0>)\n",
      "5 17 tensor(64.2823, grad_fn=<AddBackward0>)\n",
      "2 113 tensor(122677.7500, grad_fn=<AddBackward0>)\n",
      "694 615 tensor(70448.0703, grad_fn=<AddBackward0>)\n",
      "71 1 tensor(429.8342, grad_fn=<AddBackward0>)\n",
      "15 62 tensor(682.8204, grad_fn=<AddBackward0>)\n",
      "6 154 tensor(2167.9966, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(44.4583, grad_fn=<AddBackward0>)\n",
      "36 55 tensor(420.7732, grad_fn=<AddBackward0>)\n",
      "10 5 tensor(334.9377, grad_fn=<AddBackward0>)\n",
      "222 302 tensor(43786.0859, grad_fn=<AddBackward0>)\n",
      "6 30 tensor(3785.1060, grad_fn=<AddBackward0>)\n",
      "0 38 tensor(140.2232, grad_fn=<AddBackward0>)\n",
      "191 319 tensor(17525.6582, grad_fn=<AddBackward0>)\n",
      "365 381 tensor(29837.9941, grad_fn=<AddBackward0>)\n",
      "10 5 tensor(3128.2246, grad_fn=<AddBackward0>)\n",
      "0 56 tensor(2579.9531, grad_fn=<AddBackward0>)\n",
      "238 205 tensor(18144.9707, grad_fn=<AddBackward0>)\n",
      "148 44 tensor(2800.5098, grad_fn=<AddBackward0>)\n",
      "301 179 tensor(8940.0625, grad_fn=<AddBackward0>)\n",
      "28 103 tensor(1787.6240, grad_fn=<AddBackward0>)\n",
      "502 502 tensor(42930.1914, grad_fn=<AddBackward0>)\n",
      "3 4 tensor(273.1610, grad_fn=<AddBackward0>)\n",
      "0 2 tensor(1071.5271, grad_fn=<AddBackward0>)\n",
      "7 11 tensor(805.8547, grad_fn=<AddBackward0>)\n",
      "2 1 tensor(26.2382, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(283.2066, grad_fn=<AddBackward0>)\n",
      "22 89 tensor(888.8215, grad_fn=<AddBackward0>)\n",
      "24 104 tensor(1230.0305, grad_fn=<AddBackward0>)\n",
      "2 1 tensor(61.8248, grad_fn=<AddBackward0>)\n",
      "0 38 tensor(339.0675, grad_fn=<AddBackward0>)\n",
      "108 118 tensor(3341.5950, grad_fn=<AddBackward0>)\n",
      "1 53 tensor(597.1890, grad_fn=<AddBackward0>)\n",
      "86 13 tensor(2305.9548, grad_fn=<AddBackward0>)\n",
      "0 2 tensor(916.2111, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(2744.0200, grad_fn=<AddBackward0>)\n",
      "91 133 tensor(3516.0850, grad_fn=<AddBackward0>)\n",
      "53 222 tensor(5976.9771, grad_fn=<AddBackward0>)\n",
      "4 54 tensor(299.5526, grad_fn=<AddBackward0>)\n",
      "3 4 tensor(39.7519, grad_fn=<AddBackward0>)\n",
      "701 454 tensor(50587.4883, grad_fn=<AddBackward0>)\n",
      "0 5 tensor(254.6250, grad_fn=<AddBackward0>)\n",
      "2 1 tensor(2.3776, grad_fn=<AddBackward0>)\n",
      "14 47 tensor(423.9572, grad_fn=<AddBackward0>)\n",
      "164 103 tensor(29000.4004, grad_fn=<AddBackward0>)\n",
      "112 17 tensor(1712.3149, grad_fn=<AddBackward0>)\n",
      "6 11 tensor(89.3290, grad_fn=<AddBackward0>)\n",
      "325 228 tensor(12857.2979, grad_fn=<AddBackward0>)\n",
      "16 108 tensor(1814.0519, grad_fn=<AddBackward0>)\n",
      "20 9 tensor(76.8730, grad_fn=<AddBackward0>)\n",
      "130 58 tensor(2159.2039, grad_fn=<AddBackward0>)\n",
      "24 1 tensor(545.9236, grad_fn=<AddBackward0>)\n",
      "82 50 tensor(565.4896, grad_fn=<AddBackward0>)\n",
      "4 41 tensor(879.5207, grad_fn=<AddBackward0>)\n",
      "19 13 tensor(64.8117, grad_fn=<AddBackward0>)\n",
      "23 32 tensor(316.2673, grad_fn=<AddBackward0>)\n",
      "6 1 tensor(28.5008, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(3464.9172, grad_fn=<AddBackward0>)\n",
      "6 41 tensor(189.9170, grad_fn=<AddBackward0>)\n",
      "2 71 tensor(3919.8574, grad_fn=<AddBackward0>)\n",
      "15 58 tensor(646.1915, grad_fn=<AddBackward0>)\n",
      "20 101 tensor(6317.1475, grad_fn=<AddBackward0>)\n",
      "5 18 tensor(31287.8398, grad_fn=<AddBackward0>)\n",
      "150 229 tensor(4213.8340, grad_fn=<AddBackward0>)\n",
      "16 11 tensor(59.6794, grad_fn=<AddBackward0>)\n",
      "6 1 tensor(269367.7188, grad_fn=<AddBackward0>)\n",
      "0 15 tensor(248.4461, grad_fn=<AddBackward0>)\n",
      "43 246 tensor(6418.6377, grad_fn=<AddBackward0>)\n",
      "11 1 tensor(139710.4219, grad_fn=<AddBackward0>)\n",
      "1 10 tensor(8.9169, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(11577.5381, grad_fn=<AddBackward0>)\n",
      "5 54 tensor(145.4723, grad_fn=<AddBackward0>)\n",
      "6 46 tensor(171.2953, grad_fn=<AddBackward0>)\n",
      "0 1 tensor(3.0189, grad_fn=<AddBackward0>)\n",
      "37 55 tensor(696.7682, grad_fn=<AddBackward0>)\n",
      "44 7 tensor(1404.7892, grad_fn=<AddBackward0>)\n",
      "104 125 tensor(6329.7173, grad_fn=<AddBackward0>)\n",
      "1 1 tensor(230.5485, grad_fn=<AddBackward0>)\n",
      "13 12 tensor(1151.3855, grad_fn=<AddBackward0>)\n",
      "87 22 tensor(705.0332, grad_fn=<AddBackward0>)\n",
      "29 1 tensor(474.0895, grad_fn=<AddBackward0>)\n",
      "127 24 tensor(5022.9038, grad_fn=<AddBackward0>)\n",
      "112 72 tensor(1996.0775, grad_fn=<AddBackward0>)\n",
      "0 77 tensor(428.7703, grad_fn=<AddBackward0>)\n",
      "3 2 tensor(801.4271, grad_fn=<AddBackward0>)\n",
      "31 2 tensor(7481.6196, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(len_episode):\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Get the action probabilities from the policy network\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     actor_action_0,actor_dx_0,actor_dy_0, value_0 \u001b[38;5;241m=\u001b[39m model(state_0)\n\u001b[1;32m---> 22\u001b[0m     actor_action_1,actor_dx_1,actor_dy_1, value_1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     action_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n_units,\u001b[38;5;241m3\u001b[39m,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\n\u001b[0;32m     25\u001b[0m     action_0[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mactor_action_0)\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\luxai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\luxai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m, in \u001b[0;36mPolicy.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m     actor_action[unit] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_action[unit](F\u001b[38;5;241m.\u001b[39mlog_softmax(x,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     42\u001b[0m     actor_dx[unit] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_dx[unit](F\u001b[38;5;241m.\u001b[39mlog_softmax(x,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 43\u001b[0m     actor_dy[unit] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_dy\u001b[49m\u001b[43m[\u001b[49m\u001b[43munit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(x)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actor_action,actor_dx,actor_dy,value\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\luxai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\luxai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\luxai\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(n) :\n",
    "\n",
    "    values = torch.zeros(2,num_episodes,len_episode)\n",
    "    rewards = torch.zeros(2,num_episodes,len_episode)\n",
    "    log_probs = torch.zeros(2,num_episodes,len_episode)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and get the initial state\n",
    "        obs, _ = env.reset()\n",
    "        state_0 = obs_to_state(obs['player_0'])\n",
    "        state_1 = obs_to_state(obs['player_1'])\n",
    "\n",
    "        base_reward_0 = 0\n",
    "        base_reward_1 = 1\n",
    "        # Keep track of the states, actions, and rewards for each step in the episode\n",
    "\n",
    "        # Run the episode\n",
    "        for step in range(len_episode):\n",
    "\n",
    "            # Get the action probabilities from the policy network\n",
    "            actor_action_0,actor_dx_0,actor_dy_0, value_0 = model(state_0)\n",
    "            actor_action_1,actor_dx_1,actor_dy_1, value_1 = model(state_1)\n",
    "\n",
    "            action_0 = torch.zeros(n_units,3,dtype=torch.int)\n",
    "            action_0[:,0] = Categorical(logits=actor_action_0).sample()\n",
    "            action_0[:,1] = Categorical(logits=actor_dx_0).sample() - sap_range\n",
    "            action_0[:,2] = Categorical(logits=actor_dy_0).sample() - sap_range\n",
    "\n",
    "            action_1 = torch.zeros(n_units,3,dtype=torch.int)\n",
    "            action_1[:,0] = Categorical(logits=actor_action_1).sample()\n",
    "            action_1[:,1] = Categorical(logits=actor_dx_1).sample() - sap_range\n",
    "            action_1[:,2] = Categorical(logits=actor_dy_1).sample() - sap_range\n",
    "\n",
    "            log_prob_0 = torch.sum(actor_action_0[torch.arange(n_units),action_0[:,0]])\n",
    "            log_prob_0 += torch.sum(actor_dx_0[torch.arange(n_units),action_0[:,1]])\n",
    "            log_prob_0 += torch.sum(actor_dy_0[torch.arange(n_units),action_0[:,2]])\n",
    "\n",
    "            log_prob_1 = torch.sum(actor_action_1[torch.arange(n_units),action_1[:,0]])\n",
    "            log_prob_1 += torch.sum(actor_dx_1[torch.arange(n_units),action_1[:,1]])\n",
    "            log_prob_1 += torch.sum(actor_dy_1[torch.arange(n_units),action_1[:,2]])\n",
    "\n",
    "            action = dict(player_0 = np.array(action_0,dtype=int), player_1 = np.array(action_1,dtype=int))\n",
    "\n",
    "            # Take the chosen action and observe the next state and reward\n",
    "            obs, reward, truncated, done, info = env.step(action)\n",
    "            next_state_0 = obs_to_state(obs['player_0'])\n",
    "            next_state_1 = obs_to_state(obs['player_1'])\n",
    "\n",
    "            if step == 0 :\n",
    "                reward_memory = reward\n",
    "\n",
    "            if reward['player_0'] > reward_memory['player_0'] or reward['player_1'] > reward_memory['player_1'] :\n",
    "                base_reward_0 = reward_0\n",
    "                base_reward_1 = reward_1\n",
    "\n",
    "                if reward['player_0'] > reward_memory['player_0'] :\n",
    "                    base_reward_0 += victory_bonus\n",
    "                else : \n",
    "                    base_reward_1 += victory_bonus\n",
    "                reward_memory = reward\n",
    "\n",
    "\n",
    "            reward_0 = obs['player_0']['team_points'][0] + base_reward_0\n",
    "            reward_1 = obs['player_1']['team_points'][1] + base_reward_1\n",
    "                \n",
    "            # Store the current state, action, and reward\n",
    "\n",
    "            values[0,episode,step] = value_0\n",
    "            values[1,episode,step] = value_1\n",
    "\n",
    "            rewards[0,episode,step] = reward_0\n",
    "            rewards[1,episode,step] = reward_1\n",
    "\n",
    "            log_probs[0,episode,step] = log_prob_0\n",
    "            log_probs[1,episode,step] = log_prob_1\n",
    "\n",
    "            state_0 = next_state_0\n",
    "            state_1 = next_state_1\n",
    "\n",
    "    y = rewards[:,:,-1].view(2,num_episodes,1) - rewards\n",
    "\n",
    "    y = y.flatten()\n",
    "    values = values.flatten()\n",
    "    log_probs = log_probs.flatten()\n",
    "\n",
    "    policy_loss = torch.mean(log_probs * (y - values))\n",
    "    value_loss = F.mse_loss(y,values)\n",
    "\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(base_reward_0,base_reward_1,loss)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
